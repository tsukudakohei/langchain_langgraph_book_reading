# 第4章: LangChain OSS 基礎（勉強会メモ + サンプル）

この章は「LangChainの基本コンポーネントを触ってみる」ことが目的です。  
各節は `docker-check/4-x.py` の最小サンプルで動作確認できます。

## 前提

- APIキーは `.env` に `OPENAI_API_KEY` として保存します（Gitにはコミットしません）
- 任意: `OPENAI_MODEL` を設定すると使用モデルを切り替えできます（未設定なら `gpt-5-mini`）
- APIキーが無い場合でも、`DRY_RUN=1` を付けると「API呼び出しなしで」挙動の雰囲気だけ確認できます

```bash
cp .env.example .env
```

`.env` 例:

```bash
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxx
# OPENAI_MODEL=gpt-5-mini
```

Dockerで実行（推奨）:

```bash
UID="$(id -u)" GID="$(id -g)" docker compose build
```

ワンショット実行（推奨）:

```bash
sh 4-1.sh
sh 4-2.sh
sh 4-3.sh
sh 4-4.sh
sh 4-5.sh
sh 4-6.sh
sh 4-7.sh
```

## 4.1 Agents

公式: https://docs.langchain.com/oss/python/langchain/agents

### 何をする？
LLMに「ツール（関数）」を与えて、必要に応じて呼び出させる仕組みがエージェントです。  
この節では `create_agent` でエージェントを作り、足し算ツールや時刻取得ツールを呼び出します。

```bash
sh 4-1.sh
```

DRY_RUN:

```bash
DRY_RUN=1 sh 4-1.sh
```

## 4.2 Models

公式: https://docs.langchain.com/oss/python/langchain/models

### 何をする？
プロバイダ（OpenAIなど）に依存しすぎない形で、モデルを初期化して呼び出します。  
この節では `init_chat_model` の `invoke` と `batch`（まとめ呼び出し）を体験します。

```bash
sh 4-2.sh
```

## 4.3 Messages

公式: https://docs.langchain.com/oss/python/langchain/messages

### 何をする？
会話は「メッセージの配列」です。System/Human/AI などの型で扱うと、役割が明確になります。  
この節では `SystemMessage` + `HumanMessage` をモデルに渡して応答を得ます。

```bash
sh 4-3.sh
```

## 4.4 Tools

公式: https://docs.langchain.com/oss/python/langchain/tools

### 何をする？
`@tool` で「モデルが呼べる関数」を定義します。さらに、ツール実行時に `ToolRuntime` が注入されると、
state（状態）を読んだり、`Command(update=...)` で state を更新できます。

この節では:
- `tool.invoke(...)` の単体実行
- `set_user_name` が state を更新
- `show_user_name` が state を読む

を最小例で確認します。

```bash
sh 4-4.sh
```

## 4.5 Short-term-memory

公式: https://docs.langchain.com/oss/python/langchain/short-term-memory

### 何をする？
エージェントは通常「1回の呼び出し」で状態が終わりますが、`checkpointer` と `thread_id` を使うと、
同じスレッド（会話）として状態を継続できます。

この節では `InMemorySaver` を使い、同じ `thread_id` で2回呼んで「名前を覚えている」ことを確認します。

```bash
sh 4-5.sh
```

## 4.6 Streaming

公式:
- https://docs.langchain.com/oss/python/langchain/streaming/overview
- https://docs.langchain.com/oss/python/langchain/streaming/frontend

### 何をする？
長い出力を待つのではなく「生成途中の断片」を受け取りながらUIへ表示するのが streaming です。

この節は2つのモードがあります:

1. CLIでストリーミング表示（コンソール）

```bash
sh 4-6.sh
```

2. 最小フロント（HTML+JS）でストリーミング表示（SSE）

`SERVE=1` を付けると、コンテナの `8000` ポートを公開してサーバを起動します。

```bash
SERVE=1 sh 4-6.sh
```

ホスト側ブラウザで `http://localhost:8000/` を開くと、トークンが逐次表示されます。  
（公式の `frontend` ページはReact向けの例が中心なので、このリポジトリではSSEで最小体験できる形にしています）

DRY_RUNでもフロントは確認できます:

```bash
DRY_RUN=1 SERVE=1 sh 4-6.sh
```

## 4.7 Structured-output

公式: https://docs.langchain.com/oss/python/langchain/structured-output

### 何をする？
LLMの出力を「自由文」ではなく「スキーマ（構造）」として受け取ると、アプリ実装が安定します。  
この節では Pydantic のスキーマを定義して、`response_format` で構造化結果を得ます。

```bash
sh 4-7.sh
```

## よくある詰まりポイント

- `OPENAI_API_KEY` が入っていない: `.env` を確認（`cp .env.example .env`）
- `docker compose run` でサーバが見えない: `--service-ports` を付ける（Streaming節）
- モデル名を変えたい: `.env` に `OPENAI_MODEL=...` を追加

---

## 4.8 追加トピック: OpenAI Agents SDK（LangChainとの違い / ハーネス）

この勉強会では「Agent SDKのハーネス」を、次のように定義して議論します。

- `Runner` が回す agent loop（ツール呼び出し→結果を入力へ追加→再実行…）
- streaming events（生成途中のデルタ + “意味のあるイベント”）
- `final_output`（最終出力の取り出し）
- input items（Responses APIの入力items列）と `to_input_list()`（入力の成長）
- Sessions（会話履歴の自動管理）
- Tracing（観測性。必要なら無効化も可能）

詳細資料: `docs/agents_sdk.md`

サンプル実行（ワンショット）:

```bash
sh 4-8.sh
sh 4-9.sh
sh 4-10.sh
sh 4-11.sh
```

DRY_RUN:

```bash
DRY_RUN=1 sh 4-8.sh
DRY_RUN=1 sh 4-9.sh
DRY_RUN=1 sh 4-10.sh
DRY_RUN=1 sh 4-11.sh
```
