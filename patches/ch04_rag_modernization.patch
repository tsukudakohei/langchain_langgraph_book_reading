# ch04 RAG modernization patch (illustrative)
#
# 狙い: “昔のLangChain RAG例（RetrievalQA / legacy chains）” を、
#       LangChain v1 の推奨（tool + create_agent / もしくは 2-step RAG chain）に寄せる差分案。
#
# 参考（公式の方向性）:
# - RAG agent: tool で retrieval を定義して create_agent(...) でオーケストレーションする例が docs に掲載
# - 2-step chain: retrieval を毎回やって prompt に注入（middleware/dynamic_prompt 等）も docs に掲載
#
# この repo では “実コード” として docker-check/ch04/03_rag_agent_create_agent.py を追加済み。
#
diff --git a/ch04/rag.py b/ch04/rag.py
index 1111111..2222222 100644
--- a/ch04/rag.py
+++ b/ch04/rag.py
@@ -1,30 +1,66 @@
-# (old style example - pseudo)
-from langchain.chains import RetrievalQA
-from langchain.vectorstores import Chroma
-from langchain.embeddings import OpenAIEmbeddings
-from langchain.chat_models import ChatOpenAI
+# (new style - LangChain v1 recommended direction)
+# - vector store は provider split: langchain-chroma
+# - embeddings / model は provider split: langchain-openai
+# - retrieval を tool として公開し、agent(create_agent) で呼び出す
+
+from langchain_chroma import Chroma
+from langchain_openai import OpenAIEmbeddings, ChatOpenAI
+from langchain.tools import tool
+from langchain.agents import create_agent
+
+PERSIST_DIR = ".chroma/ch04"
+COLLECTION = "ch04"
+
+emb = OpenAIEmbeddings(model="text-embedding-3-small")
+vector_store = Chroma(
+    collection_name=COLLECTION,
+    persist_directory=PERSIST_DIR,
+    embedding_function=emb,
+)

-emb = OpenAIEmbeddings()
-db = Chroma.from_documents(docs, emb, persist_directory="db")
-llm = ChatOpenAI(model="gpt-3.5-turbo")
-qa = RetrievalQA.from_chain_type(llm=llm, retriever=db.as_retriever())
-print(qa.run("..."))
+@tool(response_format="content_and_artifact")
+def retrieve_context(query: str):
+    \"\"\"Retrieve information to help answer a query.\"\"\"
+    docs = vector_store.similarity_search(query, k=2)
+    serialized = "\\n\\n".join(
+        f"Source: {d.metadata}\\nContent: {d.page_content}"
+        for d in docs
+    )
+    return serialized, docs
+
+model = ChatOpenAI(model="gpt-5-mini", temperature=0)
+agent = create_agent(
+    model,
+    tools=[retrieve_context],
+    system_prompt="必要に応じて retrieve_context を使って文脈を取得して回答してください。",
+ )
+
+query = "（質問をここに）"
+for event in agent.stream(
+    {"messages": [{"role": "user", "content": query}]},
+    stream_mode="values",
+):
+    event["messages"][-1].pretty_print()
